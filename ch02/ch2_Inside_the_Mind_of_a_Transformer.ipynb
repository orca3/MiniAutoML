{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNgVzitC65olaSlXePhGKA6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/orca3/MiniAutoML/blob/main/ch02/ch2_Inside_the_Mind_of_a_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ’¡ NOTE: We will want to use a GPU to run the examples in this notebook. In Google Colab, go to Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4."
      ],
      "metadata": {
        "id": "6dL3Noaqy_7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This note\n",
        "\n"
      ],
      "metadata": {
        "id": "21x5FsWEycpw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eda-EFX3TY9o"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet transformers tiktoken transformers_stream_generator bertviz\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "import time\n",
        "\n",
        "# Unload models and clean up gpu memory cache\n",
        "def free_gpu(model):\n",
        "  if model:\n",
        "    # Removes the reference to the model's memory,\n",
        "    # making it eligible for garbage collection.\n",
        "    del model\n",
        "\n",
        "  # Release any cached GPU memory that's no longer needed.\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  # Trigger garbage collection to ensure memory is fully released.\n",
        "  gc.collect()\n"
      ],
      "metadata": {
        "id": "mAydEIQdziki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at model config, What is model.config?\n",
        "It's a configuration object that contains all the hyperparameters and settings of the model\n",
        "It defines the model's architecture, size, and behavior\n",
        "It's essentially a blueprint of how the model is structured"
      ],
      "metadata": {
        "id": "H3K16U11Z3cX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from pprint import pprint\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Print all configuration parameters\n",
        "config = model.config\n",
        "print(\"\\n=== Model Configuration Parameters ===\")\n",
        "\n",
        "# Architecture parameters\n",
        "print(\"\\nArchitecture Parameters:\")\n",
        "print(f\"Hidden size: {config.hidden_size}\")  # Size of the hidden layers\n",
        "print(f\"Number of layers: {config.num_hidden_layers}\")  # Number of transformer blocks\n",
        "print(f\"Number of attention heads: {config.num_attention_heads}\")  # Number of attention heads\n",
        "print(f\"Intermediate size: {config.intermediate_size}\")  # Size of the MLP intermediate layer\n",
        "\n",
        "# Tokenizer parameters\n",
        "print(\"\\nTokenizer Parameters:\")\n",
        "print(f\"Vocabulary size: {config.vocab_size}\")  # Size of the vocabulary\n",
        "print(f\"Maximum position embeddings: {config.max_position_embeddings}\")  # Maximum sequence length\n",
        "\n",
        "# Print model size\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"\\nModel Size:\")\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "# Model-specific parameters\n",
        "print(\"\\nModel-specific Parameters:\")\n",
        "for key, value in config.to_dict().items():\n",
        "    if key not in ['architectures', 'model_type', 'torch_dtype']:\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "# Free GPU memory\n",
        "free_gpu(model)"
      ],
      "metadata": {
        "id": "lz08JCfdZ1hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Memory footprint"
      ],
      "metadata": {
        "id": "gTDBnMpNaroQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at Model Architecture"
      ],
      "metadata": {
        "id": "bcQnyDdxYtm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "from pprint import pprint\n",
        "\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-0.5B\",\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\"  # This will automatically handle device placement\n",
        ")\n",
        "\n",
        "print(f\"\\n=== {model_name} Architecture ===\")\n",
        "print(\"\\nModel Configuration:\")\n",
        "# pprint(model.config.to_dict())\n",
        "\n",
        "print(\"\\nModel Structure:\")\n",
        "def print_module_structure(module, prefix=''):\n",
        "    for name, child in module.named_children():\n",
        "        # Skip certain internal modules for clarity\n",
        "        if name in ['_orig_mod', 'wrapped_model']:\n",
        "            continue\n",
        "\n",
        "        # Print the current module\n",
        "        print(f\"{prefix}{name}: {type(child).__name__}\")\n",
        "\n",
        "        if \"Qwen2Attention\" in name.lower():\n",
        "          print(f\"\\nFound attention module: {name}\")\n",
        "          print(f\"Type: {type(module).__name__}\")\n",
        "\n",
        "          # Print attention-specific attributes\n",
        "          if hasattr(module, 'num_heads'):\n",
        "              print(f\"Number of attention heads: {module.num_heads}\")\n",
        "          if hasattr(module, 'head_dim'):\n",
        "              print(f\"Head dimension: {module.head_dim}\")\n",
        "          if hasattr(module, 'hidden_size'):\n",
        "              print(f\"Hidden size: {module.hidden_size}\")\n",
        "          if hasattr(module, 'rotary_emb'):\n",
        "              print(f\"Has rotary embeddings: {module.rotary_emb is not None}\")\n",
        "\n",
        "        # If it's a container module (has children), recurse\n",
        "        if list(child.children()):\n",
        "            print_module_structure(child, prefix + '  ')\n",
        "\n",
        "print_module_structure(model)\n",
        "\n",
        "free_gpu(model)"
      ],
      "metadata": {
        "id": "_ztdW1kqSluW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look into details of the attention layer"
      ],
      "metadata": {
        "id": "z1AZpFJfZB4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-0.5B\",\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "print(f\"\\n=== Detailed {model_name} Attention Analysis ===\")\n",
        "\n",
        "# Find all attention layers\n",
        "attention_layers = []\n",
        "for name, module in model.named_modules():\n",
        "    #if \"attention\" in name.lower():\n",
        "    if \"Qwen2Attention\" in type(module).__name__:\n",
        "        attention_layers.append((name, module))\n",
        "\n",
        "print(f\"\\nFound {len(attention_layers)} attention layers\")\n",
        "\n",
        "# Analyze each attention layer\n",
        "for i, (name, module) in enumerate(attention_layers):\n",
        "    print(f\"\\nAttention Layer {i}: {name}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Basic information\n",
        "    print(f\"Type: {type(module).__name__}\")\n",
        "\n",
        "    # Attention parameters\n",
        "    if hasattr(module, 'num_heads'):\n",
        "        print(f\"Number of attention heads: {module.num_heads}\")\n",
        "    if hasattr(module, 'head_dim'):\n",
        "        print(f\"Head dimension: {module.head_dim}\")\n",
        "    if hasattr(module, 'hidden_size'):\n",
        "        print(f\"Hidden size: {module.hidden_size}\")\n",
        "\n",
        "    # Rotary embeddings\n",
        "    if hasattr(module, 'rotary_emb'):\n",
        "        print(f\"Rotary embeddings: {type(module.rotary_emb).__name__ if module.rotary_emb else 'None'}\")\n",
        "\n",
        "    # Attention projections\n",
        "    print(\"\\nAttention projections:\")\n",
        "    for sub_name, sub_module in module.named_children():\n",
        "        if hasattr(sub_module, 'weight'):\n",
        "            shape = sub_module.weight.shape\n",
        "            print(f\"  {sub_name}: {type(sub_module).__name__}, Shape: {shape}\")\n",
        "\n",
        "    # Additional attention-specific attributes\n",
        "    print(\"\\nAdditional attributes:\")\n",
        "    for attr_name in dir(module):\n",
        "        if not attr_name.startswith('_') and not callable(getattr(module, attr_name)):\n",
        "            try:\n",
        "                value = getattr(module, attr_name)\n",
        "                if not isinstance(value, (torch.Tensor, torch.nn.Module)):\n",
        "                    print(f\"  {attr_name}: {value}\")\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "# Print model's attention-related configuration\n",
        "print(\"\\nAttention-related configuration:\")\n",
        "config = model.config.to_dict()\n",
        "attention_config = {k: v for k, v in config.items() if 'attention' in k.lower()}\n",
        "pprint(attention_config)\n",
        "\n",
        "free_gpu(model)"
      ],
      "metadata": {
        "id": "02p-AkJ3Vl9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "90iNlAGmpp5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Memory Calculation"
      ],
      "metadata": {
        "id": "s4rkEmZRyX-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_model_memory():\n",
        "    # Constants\n",
        "    hidden_size = 896\n",
        "    num_layers = 24\n",
        "    num_heads = 14\n",
        "    intermediate_size = 4864\n",
        "    vocab_size = 151936\n",
        "    max_position = 32768\n",
        "\n",
        "    # Memory for embeddings\n",
        "    embedding_memory = vocab_size * hidden_size * 4  # 4 bytes per float32\n",
        "\n",
        "    # Memory for each transformer block\n",
        "    # Self-attention\n",
        "    qkv_memory = hidden_size * hidden_size * 3 * 4  # Q, K, V projections\n",
        "    attention_output_memory = hidden_size * hidden_size * 4  # Output projection\n",
        "\n",
        "    # MLP\n",
        "    mlp_input_memory = hidden_size * intermediate_size * 4  # First MLP layer\n",
        "    mlp_output_memory = intermediate_size * hidden_size * 4  # Second MLP layer\n",
        "\n",
        "    # Layer norms\n",
        "    norm_memory = hidden_size * 4  # Layer normalization parameters\n",
        "\n",
        "    # Total memory per layer\n",
        "    layer_memory = (qkv_memory + attention_output_memory +\n",
        "                   mlp_input_memory + mlp_output_memory +\n",
        "                   norm_memory * 2)  # 2 layer norms per block\n",
        "\n",
        "    # Total model memory\n",
        "    total_memory = (embedding_memory +\n",
        "                   layer_memory * num_layers)\n",
        "\n",
        "    # Convert to MB\n",
        "    total_memory_mb = total_memory / (1024 * 1024)\n",
        "\n",
        "    return {\n",
        "        \"Embedding Memory (MB)\": embedding_memory / (1024 * 1024),\n",
        "        \"Memory per Layer (MB)\": layer_memory / (1024 * 1024),\n",
        "        \"Total Model Memory (MB)\": total_memory_mb,\n",
        "        \"Total Model Memory (GB)\": total_memory_mb / 1024\n",
        "    }\n",
        "\n",
        "# Calculate and print memory usage\n",
        "memory_stats = calculate_model_memory()\n",
        "print(\"\\n=== Memory Usage Analysis ===\")\n",
        "for key, value in memory_stats.items():\n",
        "    print(f\"{key}: {value:.2f}\")"
      ],
      "metadata": {
        "id": "6GvJ_5A3UcaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: use bertviz library to visualize the attention result of the input prompt \"write a short introduction about US capital city\"\n",
        "from transformers import AutoTokenizer\n",
        "from bertviz import head_view\n",
        "\n",
        "# Your input text\n",
        "text = \"write a short introduction about US capital city\"\n",
        "model_name = \"Qwen/Qwen2.5-0.5B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, output_attentions=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, output_attentions=True).eval().cuda()\n",
        "\n",
        "# Tokenize input and get token strings\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
        "\n",
        "# Generate outputs with attention\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs, output_attentions=True)\n",
        "\n",
        "# Get attention weights\n",
        "attention = outputs.attentions\n",
        "\n",
        "# Use bertviz to visualize\n",
        "head_view(attention, tokens)\n"
      ],
      "metadata": {
        "id": "YjvnjqFTqr0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gvJYeROqr6r7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}